Edify Assistant

A hybrid chatbot that combines semantic search (fast FAQ lookup with TensorFlow.js) and an LLM fallback (OpenAI GPT-4o-mini with KB grounding).

Frontend (chatbot.js)

Runs in the browser with TensorFlow.js + Universal Sentence Encoder.

Finds the closest KB entry and answers instantly if similarity â‰¥ threshold.

Backend (server.js)

Node/Express server with OpenAI API.

When no close KB match is found, the frontend POSTs to /ask.

The server embeds the user query, retrieves top-k KB entries, and calls GPT-4o-mini to answer grounded in the KB.

ğŸš€ Features

ğŸ” Semantic search in browser â†’ instant KB answers.

ğŸ§  LLM fallback for out-of-scope questions.

ğŸ“¦ KB stored in JSON (kb.json) â†’ single source of truth.

ğŸ”’ API key safe â†’ only used on backend, never exposed to browser.

âš¡ Hybrid design â†’ cheap, fast, and still smart.

ğŸ“‚ Project Structure
edify-help-bot/
â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ index.html      # UI
â”‚   â”œâ”€â”€ styles.css      # Styling
â”‚   â”œâ”€â”€ chatbot.js      # Frontend semantic search
â”‚   â””â”€â”€ public.js       # (optional) KB for frontend if needed
â”œâ”€â”€ kb.json             # Knowledge base (source of truth)
â”œâ”€â”€ server.js           # Express backend w/ LLM fallback
â”œâ”€â”€ .env                # API key + config (ignored by git)
â”œâ”€â”€ .env.example        # Sample env file
â”œâ”€â”€ package.json
â””â”€â”€ README.md

âš™ï¸ Setup
1. Clone & install
git clone https://github.com/your-username/edify-help-bot.git
cd edify-help-bot
npm install

2. Configure environment

Create a .env file in the root:

OPENAI_API_KEY=sk-yourkeyhere
PORT=8787


(Keep .env secret! Add it to .gitignore.)

3. Run backend
node server.js


Should log:

Indexed <N> KB items.
LLM fallback server on :8787


Test health:

http://localhost:8787/health
â†’ { "ok": true }

4. Run frontend

Open public/index.html in a browser (or serve with Live Server / vite / any static server).

ğŸ§© Usage

Ask a question that exists in KB (e.g., â€œhow do I reset my passwordâ€).
â†’ Semantic search answers instantly.

Ask something random (â€œhow do I cook a beef stewâ€).
â†’ Falls back to backend /ask, embeds + retrieves KB, and GPT-4o-mini replies with grounded context.

ğŸ”§ Config

Threshold: in chatbot.js

const THRESHOLD = 0.47;


Lower = fallback more often, higher = fallback less often.

Top-K retrieval: in server.js

function topK(queryEmb, k = 3) { ... }


Defaults to 3 KB entries per LLM request.

ğŸ›¡ï¸ Notes

Keep .env out of git. Share .env.example for setup.

kb.json is your single source of truth. You can expand it anytime.

Frontend only uses KB for local search; backend uses KB for retrieval + LLM context.

CORS is enabled in server.js â†’ frontend and backend can run separately during dev.

ğŸ—ºï¸ Roadmap

 Add support ticket creation for unanswered queries.

 Host both frontend + backend in one Express app to avoid CORS.

 Deploy backend (Render/Heroku) and serve frontend from same origin.

 Add analytics (track queries + hits/misses).